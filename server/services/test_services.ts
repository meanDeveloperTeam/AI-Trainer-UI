import { TrainingService } from './training';
import { TrainingJob, TrainedModel } from '@shared/schema';
import path from 'path';
import { nanoid } from 'nanoid';

// Mock storage and broadcast function
const mockStorage = {
  updateTrainingJob: async (id: number, update: any) => {
    console.log(`[Mock Storage] Updated job ${id}:`, update);
  },
  createTrainedModel: async (model: TrainedModel) => {
    console.log('[Mock Storage] Created trained model:', model);
  },
  // Add other mock methods if needed by the services
};

const mockBroadcastFunction = (jobId: string, update: any) => {
  console.log(`[Mock Broadcast] Job ${jobId} update:`, update);
};

async function runTests() {
  console.log('--- Starting Service Tests ---');

  const trainingService = new TrainingService();
  trainingService.setBroadcastFunction(mockBroadcastFunction);

  // Temporarily override storage to use mock
  // This is a hack for testing, in a real scenario, dependency injection would be better
  (trainingService as any).storage = mockStorage;

  const userId = 1; // Mock user ID
  const datasetPath = path.resolve('uploads/test_dataset.csv');

  // --- Test Case 1: Ollama Training and Inference ---
  console.log('\n--- Test Case 1: Ollama Training and Inference ---');
  try {
    const ollamaJob: TrainingJob = {
      id: 1,
      userId: userId,
      jobId: nanoid(),
      framework: 'ollama',
      baseModel: 'llama3.1:8b', // Ensure this model is available in your Ollama instance
      totalEpochs: 1,
      learningRate: 0.001,
      batchSize: 4,
      contextLength: 4096,
      datasetPath: datasetPath,
      status: 'pending',
      progress: 0,
      currentEpoch: 0,
      loss: null,
      systemPrompt: null,
      config: null,
      logs: null,
      startTime: null,
      endTime: null,
      createdAt: new Date(),
      updatedAt: new Date(),
      modelPath: 'temp_path_placeholder', // Initialize with a placeholder string
    };
    console.log('Attempting Ollama training...');
    // Capture the actual model name returned by startTraining
    const ollamaModelName = await trainingService.startTraining(ollamaJob);
    console.log('Ollama training initiated successfully (check logs for progress).');

    // Use the captured model name for the trained model object
    const trainedOllamaModel: TrainedModel = {
      id: 1,
      userId: userId,
      trainingJobId: ollamaJob.id,
      name: ollamaModelName, // Use the actual model name returned
      framework: 'ollama',
      baseModel: ollamaJob.baseModel,
      modelPath: ollamaModelName, // For Ollama, modelPath is also the name
      size: 0,
      deployed: false,
      deploymentEndpoint: null,
      createdAt: new Date(),
    };

    console.log('\nAttempting Ollama streaming inference...');
    let ollamaResponse = '';
    await trainingService.testModelStream(trainedOllamaModel, 'Hello, how are you?', (chunk) => {
      process.stdout.write(chunk); // Print chunks as they arrive
      ollamaResponse += chunk;
    });
    console.log('\nOllama streaming inference complete. Full response:', ollamaResponse);

  } catch (error: any) {
    console.error('❌ Ollama Test Failed:', error.message);
  }

  // --- Test Case 2: Transformers Training and Inference ---
  console.log('\n--- Test Case 2: Transformers Training and Inference ---');
  try {
    const transformersJob: TrainingJob = {
      id: 2,
      userId: userId,
      jobId: nanoid(),
      framework: 'transformers',
      baseModel: 'microsoft/DialoGPT-medium', // Ensure this model is available for transformers
      totalEpochs: 1,
      learningRate: 0.001,
      batchSize: 2,
      contextLength: 512,
      datasetPath: datasetPath,
      status: 'pending',
      progress: 0,
      currentEpoch: 0,
      loss: null,
      systemPrompt: null,
      config: null,
      logs: null,
      startTime: null,
      endTime: null,
      createdAt: new Date(),
      updatedAt: new Date(),
      modelPath: 'temp_path_placeholder', // Initialize with a placeholder string
    };
    console.log('Attempting Transformers training...');
    await trainingService.startTraining(transformersJob);
    console.log('Transformers training initiated successfully (check logs for progress).');

    // After trainingService.startTraining, transformersJob.modelPath will be updated
    const trainedTransformersModel: TrainedModel = {
      id: 2,
      userId: userId,
      trainingJobId: transformersJob.id,
      name: `${transformersJob.baseModel.split('/').pop()}-finetuned-${Date.now()}`, // Name generated by TrainingService
      framework: 'transformers',
      baseModel: transformersJob.baseModel,
      modelPath: transformersJob.modelPath!, // Assert non-null
      size: 0,
      deployed: false,
      deploymentEndpoint: null,
      createdAt: new Date(),
    };

    console.log('\nAttempting Transformers streaming inference...');
    let transformersResponse = '';
    await trainingService.testModelStream(trainedTransformersModel, 'Hello, how are you?', (chunk) => {
      process.stdout.write(chunk);
      transformersResponse += chunk;
    });
    console.log('\nTransformers streaming inference complete. Full response:', transformersResponse);

  } catch (error: any) {
    console.error('❌ Transformers Test Failed:', error.message);
  }

  // --- Test Case 3: vLLM Inference (and no training) ---
  console.log('\n--- Test Case 3: vLLM Inference (and no training) ---');
  try {
    console.log('Attempting vLLM training (should fail)...');
    const vllmTrainingJob: TrainingJob = {
      id: 3,
      userId: userId,
      jobId: nanoid(),
      framework: 'vllm',
      baseModel: 'llama3.1-8b', // Base model for vLLM inference
      totalEpochs: 1,
      learningRate: 0.001,
      batchSize: 1,
      contextLength: 2048,
      datasetPath: datasetPath,
      status: 'pending',
      progress: 0,
      currentEpoch: 0,
      loss: null, // Added missing property
      systemPrompt: null, // Added missing property
      config: null, // Added missing property
      logs: null, // Added missing property
      startTime: null, // Added missing property
      endTime: null, // Added missing property
      createdAt: new Date(),
      updatedAt: new Date(),
      modelPath: 'temp_path_placeholder', // Initialize with a placeholder string
    };
    await trainingService.startTraining(vllmTrainingJob);
    console.error('❌ vLLM Training Test Failed: Expected an error but training succeeded.');
  } catch (error: any) {
    console.log('✅ vLLM Training Test Passed: Caught expected error:', error.message);
  }

  try {
    const trainedVLLMModel: TrainedModel = {
      id: 3,
      userId: userId,
      trainingJobId: 3,
      name: 'llama3.1-8b', // Ensure this model is available in your vLLM instance
      framework: 'vllm',
      baseModel: 'llama3.1-8b',
      modelPath: 'llama3.1-8b',
      size: 0,
      deployed: true,
      deploymentEndpoint: 'http://localhost:8000/v1/chat/completions',
      createdAt: new Date(),
    };
    console.log('\nAttempting vLLM streaming inference...');
    let vllmResponse = '';
    await trainingService.testModelStream(trainedVLLMModel, 'What is the capital of Germany?', (chunk) => {
      process.stdout.write(chunk);
      vllmResponse += chunk;
    });
    console.log('\nvLLM streaming inference complete. Full response:', vllmResponse);

  } catch (error: any) {
    console.error('❌ vLLM Inference Test Failed:', error.message);
  }

  console.log('\n--- All Service Tests Completed ---');
}

runTests();
